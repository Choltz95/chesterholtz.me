<!DOCTYPE html>
<html>

  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name = "viewport" content = "width = device-width">

  <title>Chester Holtz</title>
  <meta name="description" content="Chester Holtz's website.  I am an aspiring scientist.
">

  <link rel="stylesheet" href="/css/blog.css">

  

  <link rel="canonical" href="http://chesterholtz.me/blog/">
  <link rel="alternate" type="application/rss+xml" title="Chester Holtz" href="http://chesterholtz.me/blog/feed.xml" />

  <!-- fonts.com -->
  <script type="text/javascript">
    // first, create the object that contains
    // configuration variables
    MTIConfig = {};

    // next, add a variable that will control
    // whether or not FOUT will be prevented
    MTIConfig.EnableCustomFOUTHandler = true // true = prevent FOUT
  </script>
  <script type="text/javascript" src="http://fast.fonts.net/jsapi/3ac768d8-7a5c-4fd8-b377-f61d7f1760fa.js"></script>

  <!-- prettify -->
  <link href="/css/prettify.css" type="text/css" rel="stylesheet" />
  <script type="text/javascript" src="/javascript/prettify.js"></script>
  <script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
</head>


  <body onload="prettyPrint()">

    <div id="container" class="group">

  <h1>
    <a class="site-title" href="/blog/">Chester Holtz</a>
  </h1>

  <div class="navigation-links">
    <a href="/">About</a>
    <span id="desktop-only">
      &nbsp;
      <a href="/blog/archive.html">Archive</a>
      &nbsp;
      <a href="/blog/feed.xml">RSS</a>
    </span>
  </div>


    <div class="page-content">
      <div class="wrapper">
        <div id="content">

  
    <div class="main">
      <h2><a href="/blog/post/stable_matching">Gale-Shapely Algorithm and the Stable Marriage Problem</a></h2>

      <div class="main"><p>This semester I am taking csc 282 - a course on the design and analysis of efficient algorithms. In this class, we analyze various popular algorithms and techniques used in the computer science including greedy/dynamic algorithms, divide-and-conquer, graph algorithms, linear programming, and np algorihtms. During the the graph section, we covered an interesting class of algorithms which dealt with a particular type of graphs called matchings. By definition, matchings are graphs whose edges do not share a common verticies. A simple way to determine whether or not a grpah is a matching is to see if it is colorable. $$x = {-b \pm \sqrt{b^2-4ac} \over 2a}.$$</p>

<p>On another note, I recently got a free chromebook and wanted to take a break from my personal research write something simple to put on the webstore. Check out my app redd reader at https://chrome.google.com/webstore/detail/redd-reader/pbhecjpnbjokeahpnaifpbjihffikbpd and feel free to leave comments and suggestions here: https://github.com/Choltz95/reddit-news-for-chrome/issues.</p>
</div>

      <div class="post-separator">&nbsp;</div>

      <br/>
    </div>
  
    <div class="main">
      <h2><a href="/blog/post/image_estimation">Image Estimation with Finite Polygons</a></h2>

      <div class="main"><p>This project was inspired by <a href="http://rogeralsing.com/2008/12/07/genetic-programming-evolution-of-mona-lisa/">Richard Alsing&#39;s</a> blog post on developing a simple evolutionary algorithm to generate an image estimation of the Mona Lisa with a finite number of polygons. Provided even a naive algorithm which consisted of a population size of two - parent and child - and unformly random mutation, the generated images proved surprisingly accurate (after a number of generations had been processed). Here is the simplified algorithm implimented by Richard Alsing in C#. </p>

<ol>
<li>Generate an initial population of programs</li>
<li>Take the n best programs of the current population</li>
<li>Create Children from the best programs by mating and mutating them</li>
<li>Replace the current population with the n-best and the children programs</li>
<li>Repeat from 2 until satisfied</li>
</ol>

<p>Recently, I have been interested in rendering graphics in C and had only completed a few tutorials for OpenGL. This project is my first independent project utilizing OpenGL and the freeGlut toolkit  to render the polygons to a window.</p>

<p>In terms of linear algebra, we can say an image is defined as a matrix of pixels. For the sake of simplicity, assume that the ith, jth pixel in an image can be either black or white - with the correspnding ith, jth element in the matrix being a 0 or 1. We can say there are both complex images and simple images. complex images - such as the mona lisa - are as you would expect. Simple images are matricies which represent a single polygon. Polygons are closed, have a variable number of verticies, and are shaded - ie the pixels within the bounds created by segmenting lines between verticies are black. Our goal is as follows: given a target complex image and positive numbers n and m, we would like to manufacture &lt;= n simple images with &lt;= m verticies such that the complex image defined as the matrix-sum of all the simple images is as close as possible to the target image. There are a number of metrics we can use to define &quot;closeness&quot;, but the one I use is simply the standard euclidean distance formula. Instead of implimenting a polygon as a matrix, however, we consider each polygon as a k-tuple &quot;DNA&quot; structure containing various identifying features and let opengl handle their rendering.</p>

<p>DNA of each polygon is represented as a tuple containing data representing vertex coordinates, color, and transparency. Initially, for each polygon, random coordinates are chosen for a single vertex, and the coordiantes for the remaining vertecies are chosen also randomly, but bounded above and below to prevent drastically large or small polygons. Mutation then takes place and for each mutation, opengl grabs pixel data and compares it to a matrix of pixels in memory representing the target image. The algorithm itterates through each pixel applying the square of the distance function to each color value (red, blue, green) before summing the result. This value represents the pixel fitness. We accumulate a sum of these pixel fitness values for every pixel indecie of the target image to aquire the child generation fitness. We then compare the child generation fitness of the child to the parent fitness. If the fitness of the child is less than or equal to the fitness of the parent, the parent dies and the child becomes the parent before &quot;bearing&quot; a mutated child. Through this naive process, the estimation image evolves over time as the parent becomes more and more similar to the target. </p>

<p>There is really not too much else to comment on. The code is on my <a href="https://github.com/Choltz95/image_estimation">github</a>. The biggest problems I have encountered - disregarding difficulties learning to use opengl and the libpng library - are with the speed of the program. By far, the least efficient component of the program is the naive image comparison algorithm given below. Initially, I had difficulties reading pixel data from the opengl render and instead system called scrot and read the image into memory with libpng. When I switched to using OpenGl&#39;s read pixels, the reduction in the number of references to libpng functions and system calls resulted in a 3x decrease in rendering and comparing generations.</p>

<pre class="prettyprint linenums">
//image comparison
for (y=0; y < target_image_height; y++) {
    png_byte* row1 = row_pointers_1[y];
    png_byte* row2 = row_pointers_2[y];
    for (x=0; x < target_image_width; x++) {
        png_byte* ptr1 = &(row1[x*3]);
        png_byte* ptr2 = &(row2[x*3]);
        dr = (ptr1[0] - ptr2[0]);
        dg = (ptr1[1] - ptr2[1]);
        db = (ptr1[2] - ptr2[2]);
        dr2 = dr * dr;
        dg2 = dg * dg;
        db2 = db * db;
        pixel_fitness = dr2 + dg2 + db2;
        current_fitness += pixel_fitness;
    }
}
</pre>

<p>I think I am done for now with this project, but it would be interesting to experiment with other metrics of image similarity. I noticed github&#39;s own document similarity metric includes images. It might be fun to look up what algorithm they use. I included some useful links on the github page as starting points for reading and will add fun screenshots to this post later.</p>
</div>

      <div class="post-separator">&nbsp;</div>

      <br/>
    </div>
  
    <div class="main">
      <h2><a href="/blog/post/predprey_1">Emergent Behavior Through Boids Simulation(1)</a></h2>

      <div class="main"><p>Simulation of natural phenomena is something I have been interested in for a long time. In particular, approximating the aggregate moving and interaction of a group of individuals is something I have thought about ever since reading Michael Chriton&#39;s <a href="https://en.wikipedia.org/wiki/Prey_(novel)">Prey</a> and about the main character&#39;s PredPrey software. A few years ago a few friends and I wrote a program based around the rules defined by <a href="http://www.red3d.com/cwr/">Craig Reynolds</a> regarding the behavior of birds engaged in flocking behaviors in response to a predator. Reynolds proposed this model of coordinated animal motion in 1986 and labled the individuals composing a simulated flock <a href="https://en.wikipedia.org/wiki/Boids">&quot;Boids&quot;</a></p>

<p>Essentially, the behavior of these Boids is subject to three fundamental rules: cohesion, separation and alignment. The beauty of Reynold&#39;s model is the simplicity of these rules, and the resulting complexity of the Boid&#39;s behavior. Reynold&#39;s model gave rise to the concept of emergent behavior: the complex interaction of individual agents adhering to a set of simple rules in an artificial intelligent simulation.</p>

<p>I coded this project in a weekend in about 300 lines using the processing language with the processing.js library as a way to visualize the three rules in a browser. In predprey, each rule is implimented as a force vector and computed for each boid based on the location (also a vector) of surrounding boids. Along with the canonical behavioral rules, I also implimented more advanced ones such as response to predators, obstacles, exhaustion, and goalfinding.</p>

<p>Before implimenting the actual rules, I first wrote a reusable method to return data from surrounding agents. The radius parameter refers to the radius around a particular boid for the method to get location and velocity vectors from. Depending on the number of boids within the radius, the method will then compute the average location (center of mass) or average velocity of the boids within the radius.</p>

<pre class="prettyprint linenums">
PVector compute_com(ArrayList<Boid> boids, int view_rad, boolean v) {
    float count = 0; // Keep track of how many boids are too close.
    PVector vec_sum = new PVector();

    for (Boid other: boids) {
      int separation = mass + view_rad;
      PVector dist = PVector.sub(other.getLoc(), loc); // distance to other boid.
      float d = dist.mag();

      if (d != 0 && d<separation) { // If closer than desired, and not self.
        PVector other_vec = new PVector();
        if(v) { other_vec = other.getVel(); } // if we want to average the velocities vs. locations
        else { other_vec = other.getLoc(); }
        vec_sum.add(other_vec); // All locs from closeby boids are added.
        count ++;
      }
    }
    vec_sum.div(count);
    if(count > 0) {
      return vec_sum;
    } else {
      return null;
    }
 }
</pre>

<p>From here, implimentation of Reynold&#39;s fundamental rules is trivial, and  the rule for separation is provided below. In my implimentation, the max_force coefficients and radii are determined arbitrarily for more &quot;natural&quot; behavior. The boids are also colorized to represent the different state transitions.</p>

<pre class="prettyprint linenums">
void separate (ArrayList<Boid> boids) {
    PVector com = compute_com(boids, 20, false);
    if(com != null) {
      color[2]+=2;
      PVector avoidVec = PVector.sub(loc, com);
      avoidVec.limit(max_force*2.5); // Weigh by factor arbitrary factor 2.5.
      apply_force(avoidVec);    
    }
}
</pre>

<p>Boids is an interesting little program that serves as the initial example of emergent behavior from three simple rules. For fun, I added a little predator that the boids flee from. Although there are few applications of these kinds of simulations, they are fun to code and entertaining to watch.</p>

<p>If I have time in the future, I would love to expand this program to encompass the third dimention and I also would like to impliment a genetic algorithm to effectively have the boids &quot;evolve&quot; and develop behaviors other than the ones I decide.</p>
</div>

      <div class="post-separator">&nbsp;</div>

      <br/>
    </div>
  
    <div class="main">
      <h2><a href="/blog/post/rpi_distributed_1">Distrbuted computation with a raspberry pi cluster(1)</a></h2>

      <div class="main"><p>I have had a great deal of fun reading about the many adventures of the physicist <a href="https://en.wikipedia.org/wiki/Richard_Feynman">Richard Feynman</a>. I read his semi-autobiography <em>Surely You&#39;re Joking Mr. Feynman!</em> as a kid, watched his video series on computer heuristics as a freshman taking a class on organization of computer systems, and found his physics notes to be invaluable while taking my university&#39;s freshman physics series. Recently I have also been reading about his collaboration with the great computer scientist <a href="https://en.wikipedia.org/wiki/Danny_Hillis">Dany Hillis</a> - coincidentally advised by 3 other famous computer scientists - on the <a href="https://en.wikipedia.org/wiki/Connection_Machine">Connection Machine</a> - a parallel arangement of multiple supercomputers. Additionally, one of my mentors at The University of Rochester just retired, and while browsing his library, I found that he had worked on the development of software for the <a href="https://en.wikipedia.org/wiki/BBN_Butterfly">BBN Butterfly Processor</a> - one of the largest parallel computers of the 1980s. These factors all served to exite my interest in parallel computation and distributed systems. Although it is a topic I am most interested in, I will most likely not be taking a class on the topic, but also want some sort of foundation in the subject. </p>

<p>In this project series I will be experimenting with various distributed algorithms for doing various things over a network. I will be examining the efficiency of computation which can be divided among multiple processors. Algorithms which I will be looking at include the canonical mergesort, matrix arithmetic, plotting a Delaunay Triangulation, and others.</p>

<p>For testing and comparing the parallelized and single-processor approaches for various aglorithms, I primarily consider one important metric: average computation time. These statistics are gathered during run time by using the high precision timers in the C++ chrono library, which allows the collection of timing data accurate on the order of nanoseconds. Testing itself is performed on three raspberry Pi B+ computers. These machines feature an 700 MHz single-core ARM1176JZF-S processor with 144 KB of Cache and 512 MB of RAM. The complete code for this project can be found on on my <a href="https://github.com/Choltz95/distributedRPI">github</a></p>

<p>Merge sort is a classic sorting algorithm used to introduce the divide and conquer algorithm design paradigm. As such, it is known to parallelize well.</p>

<p>Recursively, mergesort processes an unsorted list of numbers by dividing the unsorted list into n sublists, each containing 1 element (a list of 1 element is considered sorted). It then repeatedly merges sublists to produce new sorted sublists until there is only 1 sublist remaining. This will be the sorted list. The following is a short walkthrough of the server-side code written for a distributed mergesort and the results of the comparison.</p>

<p>An general example is given below with some merge and split steps skipped to conserve space. 
<pre class="prettyprint linenums">
Start       : 3--4--2--1--7--5--8--9--0--6
Split       : 3--4--2--1--7  5--8--9--0--6
Split       : 3  4  2  1  7  5  8  9  0  6
Merge       : 3--4  1--2  5--7  8--9  0--6
Merge       : 1--2--3--4  5--7--8--9  0--6
Merge       : 0--1--2--3--4--5--6--7--8--9
</pre></p>

<p>I partition my initial unsorted array into n subarrays of equal size on the server. These subarrays will be passed to individual raspberry pi nodes to be sorted before being merged on the server. My breakarray() funtion intuitively takes the initial unsorted array and the number of processors on the network.
<pre class="prettyprint linenums">
def breakarray(array, n): 
    sectionlength = len(array)/n    #length of each section 
    result = [] 
    for i in range(n):
        if i &lt; n - 1:
            result.append( array[ i * sectionlength : (i+1) * sectionlength ] )
        #include all remaining elements for the last section 
        else:
            result.append( array[ i * sectionlength : ] )<br>
    return result
</pre></p>

<p>Furthermore, setting up a simple network is quite simple. I make use of Python&#39;s socket module to do this. I first provide host and port parameters and create an inet, streaming socket before binding the socket to local host and port.
<pre class="prettyprint linenums">
HOST = &#39;&#39;
PORT = 50007 
s = socket.socket(socket.AF<em>INET, socket.SOCK</em>STREAM) 
print &#39;[DEBUG] Socket created&#39;
s.setsockopt(socket.SOL<em>SOCKET, socket.SO</em>REUSEADDR, 1) 
try:
    s.bind((HOST, PORT))
except socket.error as msg:
    print &#39;[ERROR] Bind failed. Error Code : &#39; + str(msg[0]) + &#39; Message &#39; + msg<a href="https://en.wikipedia.org/wiki/Richard_Feynman">1</a>
    sys.exit()
</pre></p>

<p>Finally, I send and recieve the data in 4KB size chunks. Once sorted, the client sends its assigned subarray back to the server to be merged.
<pre class="prettyprint linenums">
for i in range(procno - 1): # Converts array section into string to be sent
    arraystring = repr(sections[i+1]) 
    conn.sendto(arraystring, addr_list[i])  # Sends array string 
print &#39;[DEBUG] Data sent, sorting array...&#39;
</pre></p>

<pre class="prettyprint linenums">
# Receives sorted sections from each client
for i in range(procno - 1):
    arraystring = '' 
    print '[DEBUG] Receiving data from clients...' 
    while 1:
        data = conn.recv(4096)  # Receives data in chunks 
        arraystring += data # Adds data to array string 
        if ']' in data: # When end of data is received
            break

    print '[DEBUG] Data received, merging arrays...'    
    array = ms.merge(array, eval(arraystring))  # Merges current array with section from client
    print '[DEBUG] Arrays merged.'
</pre>

<p>Tests were preformed on list lengths ranging from 1,000 to 1,000,000, and in all cases, the distributed set up outpreformed the single-node settup by factor seemingly proportional to the number of nodes I distributed the unsorted list accross. Sample output is given below:</p>

<pre class="prettyprint">
$ sudo python server.py 3 10000
[DEBUG] Waiting for client...
[DEBUG] connected to 192.168.1.2:50007
[DEBUG] connected to 192.168.1.3:50007
[DEBUG] Data sent, sorting array...
[DEBUG] Array sorted.
[DEBUG] Receiving data from clients...
[DEBUG] Data Recieved, merging arrays...
[DEBUG] Arrays merged.
[DEBUG] Time taken to sort: 21.223145 seconds.
</pre>

<p>This concludes the first part of this project series. Expect more updates coming soon.</p>
</div>

      <div class="post-separator">&nbsp;</div>

      <br/>
    </div>
  
    <div class="main">
      <h2><a href="/blog/post/personal_statement">Introduction and personal statement</a></h2>

      <div class="main"><p>I am a junior at <a href="http://www.hajim.rochester.edu/">The University of Rochester Hajim School of Engineering</a> studying computer science and mathematics. I am interested in investing myself in projects which present challenges relating several sciences or technologies. I am currently seeking a summer internship, research position, or job in computer science.</p>

<p>I would like to continue a path of research in big data analysis and in developing software to interface with real-world data. Currently I am working on writting real time mapping visualization of occurances of global chaotic events. </p>

<p>Along with computer science, I also have a love of physics and mathematics and I am especially interested in the application of game theory to political science. </p>

<p>My previous website served as a portfolio to demonstrate my competence in several web technologies and frameworks, but grew to be quite bloated and unecessarily flashy. I decided to focus less on the website itself and instead to make a website which contained deep and meaningful content.</p>

<p>Since I am not much of a writter, updates to the blog portion of this website will not be consistent, and will most likely be composed of technical reports of projects and research I am involved in. I definitely do not expect this website to be visited frequently, and am primarily utilizing this blog to organize my thoughts and keep a well layed out map of projects and ideas I find interesting.</p>

<p>On my previous website, I had a page devoted to my five favorite books. They are:   </p>

<ul>
<li><p><a href="https://en.wikipedia.org/wiki/G%C3%B6del,_Escher,_Bach">Gödel, Escher, Bach: An Eternal Golden Braid</a></p></li>
<li><p><a href="https://en.wikipedia.org/wiki/Do_Androids_Dream_of_Electric_Sheep%3F">Do Androids Dream of Electric Sheep?</a></p></li>
<li><p><a href="https://en.wikipedia.org/wiki/Dune_(novel)">Dune</a></p></li>
<li><p><a href="https://en.wikipedia.org/wiki/Surely_You%27re_Joking,_Mr._Feynman!">Surely You&#39;re Joking, Mr. Feynman!: Adventures of a Curious Character</a></p></li>
<li><p><a href="https://en.wikipedia.org/wiki/The_Lord_of_the_Rings">Lord of the Rings</a></p></li>
</ul>
</div>

      <div class="post-separator">&nbsp;</div>

      <br/>
    </div>
  
    <div class="main">
      <h2><a href="/blog/post/finite_population_visualization">Visualizing the genetic diversity of finite populations</a></h2>

      <div class="main"><p>The <a href="https://en.wikipedia.org/wiki/Moran_process">Moran Process</a> is a simple stochastic process used to describe finite populations[wikipedia].
I created a simply visualization of fundamental properties of an evolving population. Since color can be represented as a 6-digit hexadecimal number, we can generalize color to be representative of an agents DNA, and preform operations representing mutation, inheritence, selection, and crossover by altering the color of the agent.</p>
</div>

      <div class="post-separator">&nbsp;</div>

      <br/>
    </div>
  
    <div class="main">
      <h2><a href="/blog/post/event_driven_cartography">Evis - event driven cartography with the GDELT dataset</a></h2>

      <div class="main"><p>The progress on this project has been relatively slow, since i have been working on this project inconsistently. My interest in analysis of large data sets is due to my involvement in Professor Jiebo Luo&#39;s <a href="http://www.cs.rochester.edu/u/jluo/">VIStA (Visual Intelligence &amp; Social Multimedia Analytics) Research Group</a> as a research assistant. Currently, I am developing a web interface for the <a href="http://gdeltproject.org/">gdelt (Global Database of Events, Language, and Tone) dataset</a> while also preforming analysis and rendering a visualization of real-time chaotic events - natural disasters, violence against civilians, etc. - in the world. </p>

<p>Gdelt is a continuously updated repository which gathers events from around the world since 1979 and indexes them according to various metrics. Every 15 minutes, GDELT scrapes various news sources for relavent data and current events to update its database with entries containing locations, times, descriptions and other relavent data.</p>

<p>In testing, I use Python&#39;s matplotlib module with the basemap toolkit to plot data in real time and to literally draw correlations between events. Recently, GDELT was uploaded to google&#39;s bigquery which allows me to parse and analize over a quarter million events for free in seconds.</p>

<p>For the final rendition, I plan writting the application in javascript to be renderable in a browser.</p>

<p>The code is hosted on my <a href="https://github.com/Choltz95/Evis">github</a>, but is probably a little behind the current version as I am working on this project in a cloud vm.</p>

<p>An example demonstrating the power of google bigquery is provided below (taken from google&#39;s cloud platform blog): when I run the following sql query against a databse containing a quarter billion rows a set of 37 entries representing the most significant events of the last 37 years is returned in only a few seconds. From there, I can download the data as a json file and parse it, drawing data onto a map to be interacted with, or simply examined.
<pre class="prettyprint linenums">
SELECT Year, Actor1Name, Actor2Name, Count FROM (
SELECT Actor1Name, Actor2Name, Year, COUNT(*) Count, RANK() OVER(PARTITION BY YEAR ORDER BY Count DESC) rank
FROM 
(SELECT Actor1Name, Actor2Name,  Year FROM [gdelt-bq:full.events] WHERE Actor1Name &lt; Actor2Name and Actor1CountryCode != &#39;&#39; and Actor2CountryCode != &#39;&#39; and Actor1CountryCode!=Actor2CountryCode),  (SELECT Actor2Name Actor1Name, Actor1Name Actor2Name, Year FROM [gdelt-bq:full.events] WHERE Actor1Name &gt; Actor2Name  and Actor1CountryCode != &#39;&#39; and Actor2CountryCode != &#39;&#39; and Actor1CountryCode!=Actor2CountryCode),
WHERE Actor1Name IS NOT null
AND Actor2Name IS NOT null
GROUP EACH BY 1, 2, 3
HAVING Count &gt; 100
)
WHERE rank=1
ORDER BY Year
</pre></p>

<pre class="prettyprint">
{"Year":"1979","Actor1Name":"CHINA","Actor2Name":"VIETNAM","Count":"2668"}
{"Year":"1980","Actor1Name":"AFGHANISTAN","Actor2Name":"RUSSIA","Count":"3899"}
{"Year":"1981","Actor1Name":"RUSSIA","Actor2Name":"UNITED STATES","Count":"3079"}
{"Year":"1982","Actor1Name":"ISRAEL","Actor2Name":"LEBANON","Count":"4253"}
{"Year":"1983","Actor1Name":"ISRAEL","Actor2Name":"LEBANON","Count":"4955"}
...
{"Year":"2012","Actor1Name":"CHINA","Actor2Name":"UNITED STATES","Count":"42231"}
{"Year":"2013","Actor1Name":"RUSSIA","Actor2Name":"UNITED STATES","Count":"61191"}
{"Year":"2014","Actor1Name":"RUSSIA","Actor2Name":"UKRAINE","Count":"120995"}
{"Year":"2015","Actor1Name":"RUSSIA","Actor2Name":"UKRAINE","Count":"39236"}
</pre>

<p><img src="/images/map.png" alt="alt text" title="Example map"></p>

<p>The biggest issue encountered is the free data limit. Since the goal for this project involves a real-time component, it is necessary to requery data from gdelt every update.  </p>
</div>

      <div class="post-separator">&nbsp;</div>

      <br/>
    </div>
  
    <div class="main">
      <h2><a href="/blog/post/micro_lisp_interpreter">Micro Lisp interpreter and garbage collection</a></h2>

      <div class="main"><p>This past semester I have been rereading Gerald Sussman&#39;s popular text: Structure and Interpretation of Computer Programs. I have worked on a lisp interpreter in JavaScript in the past inspired by the implementations of <a href="https://en.wikipedia.org/wiki/Peter_Norvig">Peter Norvig</a> and <a href="https://github.com/maryrosecook">Mary Rose Cook</a> - one that followed the 10 rules of <a href="https://en.wikipedia.org/wiki/John_McCarthy_(computer_scientist)">John McCarthy</a> detailed in his paper <a href="http://www.cse.sc.edu/%7Emgv/csce330f13/micromanualLISP.pdf">A Micro-Manual for Lisp - Not the whole Truth</a>. This implementation is what will be discussed in this first blog post. In a future post, I may go into detail regarding my eventual reimplimintation of Micro Lisp and several garbage collection algorithms in C++.</p>

<p>LISP is family of programming languages first conceived in 1959 by John McCarthy. In LISP, computation is expressed as a function of one or more objects. Objects can be other functions, data types, or data structures. Despite its age, derivations of LISP such as Common Lisp, Clojure, and Scheme are the most commonly used programming languages for AI research and many other applications.</p>

<p>An interpreter is a program that evaluates instructions written in a programming langauge. In contrast to a compiler, an interpreter remains present for the durration of code execution. In general, there are three fundamental phases of interpretation - tokenization, parsing, and evaluation.</p>

<p>My <a href="http://github.com/Choltz95/microlispjs">Micro Lisp</a> is an interpreter that supports function invocation, lambdas, lets, ifs, numbers, strings, a few JavaScript library functions, and lists. I wrote it over a weekend in about 200 lines of JavaScript, and also included a number of simple and more complex test cases. The code for the project can be found on my <a href="http://github.com/Choltz95/microlispjs">github</a>, while one can test a deployed version <a href="http://littlelispjs.divshot.io/">here</a>. It is recommended, however, to clone yourself a copy directly from the repository and open index.html in your browser locally as there might be some features - like multiline coding - that are only supported in the current version on github.</p>

<p>The two primary parts of interpretation I focused on when writting my Lisp interpreter where the <code>parsing</code> and <code>evaluation</code> of code. When we parse a Lisp expression, we take the code typed by the programmer and transform  it into a representation that we can traverse and evaluate. Evaluation refers to the procedure of processing this structure according to the symantic rules of Lisp and returning a result.</p>

<p>Traditionally, the semantic parsing process is separated into two parts: the <code>tokenization</code> and the assembling of the <code>AST</code>. </p>

<p>The tokenizer demarcates a string of input characters into <code>tokens</code> before passing them on to be assembled into an AST - which will be defined later. For Micro Lisp, tokens consist of parentheses, symbols, and numbers.</p>

<p>We tokenize by taking advantage of JavaScript&#39;s <code>replace()</code> and <code>split()</code> functions to take a character string input, add whitespace around each parentheses, and split the result by whitespace to get a JavaScript list of tokens. tokenize() is given below.</p>

<pre class="prettyprint linenums">var tokenize = function(input) {
   return input.replace(/\(/g, ' ( ')
               .replace(/\)/g, ' ) ')
               .trim()
               .split(/\s+/);
 };
</pre>

<p>An AST, or Abstract Syntax Tree is a representation of the structure of code written in a language. The tree is abstract since each node of this tree represents a construct in code, but some elements of the code may be ommitted, i.e. parentheses in our case. Since the inherint syntax structure of a lisp symbolic expression - &#39;S-Expression&#39; - is representative of an AST, this task is quite simple. An S-Expression can simply be defined inductively as an atom, or an expression (x y) where x and y can be S-Expressions themselves.</p>

<p>Atoms are collections of letters, digits or other characters not otherwise defined in the micro-lisp language. furthermore, lists consist of a left parenthesis followed by a head - or <code>CAR</code> - and a tail - a <code>CDR</code>. Lists always end with a closing parenthesis.</p>

<p>Parsing a Lisp S-Expression is quite simple. First, parse() is called with a character string input representing the program. to be interpreted. We then tokenize this input to get a list of tokens and pass this list into read_from to assemble the AST. We shift through elements of the list one at a time. If the token at the 0th indice is a &#39;(&#39;, we instantiate a list of S-Expressions and recursively add to that list until we encounter a matching &#39;)&#39;. The parse() logic is given below.</p>

<pre class="prettyprint linenums">
function parse(input) {
  return read_from(tokenize(input));
}

function read_from(tokens) {
    if (tokens.length == 0) {
        throw 'unexpected EOF';
    }

    var token = tokens.shift();
    if ('(' == token) {
        var L = [];
        while (tokens[0] != ')') {
            L.push(read_from(tokens));
        }
        tokens.shift();
        return L;
    } else {
        return atom(token);
      }
}
</pre>

<p>Below is an example of input, and the resulting output of parsing the Lisp S-Expression (hello (hello world)).</p>

<pre class="prettyprint">
>tokenize('(hello (hello world))')
["(", "hello", "(", "hello", "world", ")", ")"]
</pre>

<pre class="prettyprint">
>parse('(hello (hello world))')
["hello", ["hello", "world"]]
</pre>

<p>Evaluation is the most complex part of the interpretation process. When we evaluate, we look at an expression and check its value according to the <code>env</code> environment - implemented as a JS Dictionary. An environment is simply a mapping from a variable name to its value. </p>

<p>In code, we impliment a finite number of predefined functions in the global environment as</p>

<pre class="prettyprint linenums">
var Operations = {
'+'       : function(a, b) { return a + b; },
'-'       : function(a, b) { return a - b;},
'*'       : function(a, b) { return a * b; },
'/'       : function(a, b) { return a / b; },
'<'       : function(a, b) { return a < b; },
'>'       : function(a, b) { return a > b; },
'<='      : function(a, b) { return a <= b; },
'>='      : function(a, b) { return a >= b; },
'='      : function(a, b) { return a == b; },
'or'     : function(a,b)  { return a||b;   },
'cons'   : function(a, b) { return [a].concat(b); },
'car'    : function(a)    { return (a.length !==0) ? a[0] : null; },
'cdr'    : function(a)    { return (a.length>1) ? a.slice(1) : null; },
'list'   : function()     { return Array.prototype.slice.call(arguments); },
};
</pre>

<p>and impliment a second environment - or symbolic table when the interpreter parses a <code>lambda</code> or <code>def</code> function. This second environment typically contains information pertaining to expressions and variables such as their scope or type.</p>

<p>When we evaluate an expression, we check the expression&#39;s function and arguments. If an expression is prefixed by a <code>&#39;</code>, or the function being applied is the string &quot;QUOTE&quot;, we return the expression, or arguments literally and do not evaluate. The forms necessary for a lisp to be considered a Micro Lisp are given as the rules below where expressions are denoted e or a, functions as f and variables as v.</p>

<ol>
<li>QUOTE - The value of (QUOTE A) is A</li>
<li>CAR - The value of (CAR e) is the first element of e where e is defined as a non-empty list. i.e. (CAR (QUOTE (A B))) returns A</li>
<li>CDR - The value of (CDR e) is the list of remaining elements of e when CAR of e is removed, where e is defined above. ie (CDR (QUOTE (A B))) returns B.</li>
<li>CONS - The value of (CONS e1 e2) is the list that results from prefixing e1 onto e2. Thus, (CONS (QUOTE A) (QUOTE B)) returns the list (A B).</li>
<li>EQUAL - The value of (EQUAL e1 e2) is true if e1 = e2 and false if otherwise. (EQUAL 1 2) returns false.</li>
<li>ATOM - The value of (ATOM e1) is true if e1 is an atom and false if otherwise. (ATOM)</li>
<li>COND - The value of (COND(e1 e1) ... (pn en)) is the value of ei, where pi is the the first ofthe p&#39;s whose value is not NIL.</li>
<li>DEFINE - The DEFINE function maps a variable to the given expression.</li>
<li>LAMBDA - Lambda is a construct in the Micro-Lisp language which allows for the definition
of anonymous functions.</li>
<li>Operators - We also provide definitions for traditional boolean ( =, &gt;, ≥, &lt;, ≤...) and
arithmetic operators (+, −, ∗, /...)</li>
</ol>

<p>An example of the case &#39;DEFINE&#39; is given below:</p>

<pre class="prettyprint linenums">
case "DEFINE":
    [_, variable, exp] = x;
    env.set(variable, evaluate(exp, env));
</pre>

<p>Here, we switch over the first token x[0], and assuming that the next token x[1] is an atom, we set a new indice in the local environment to the variable name &#39;variable&#39; and its value to the argument expression (x[2]) recursively evaluated with respect to the global enviornment.</p>

<p>With these rules in place, we have a robust and portable lisp that we can use to program anywhere in with a browser. It becomes quite easy to define our own, more complex functions. By querying the help function by typing &quot;sample&quot; into the interpreter prompt, a number of different examples are presented with the most advanced being the recursive application of the fibonacci function onto a range of numbers.</p>

<pre class="prettyprint">
> (define range (lambda (a b) (cond (= a b) (quote ()) (cons a (range (+ a 1) b)))))
null
> (define map (lambda (f xs) (cond (= xs nil) nil (cons (f (car xs)) (map f (cdr xs))))))
null
> (define fib (lambda (n) (cond (or (= n 0) (= n 1)) 1 (+ (fib (- n 1)) (fib (- n 2))))))
null
> (map (lambda (x) (fib x)) (range 0 10))
(1 1 2 3 5 8 13 21 34 55 null)
</pre>

<p>Originally I had intended to also discuss preforming garbage collection on list based system in this post and implementing a turtle graphics module utilizing HTML5 canvas, but I think I will visit those topics at a later time. </p>
</div>

      <div class="post-separator">&nbsp;</div>

      <br/>
    </div>
  

  <div class="navigation group mti_font_element">
    

    
	</div>

</div>

      </div>
    </div>

    </div>

<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-24453347-1', 'auto');
  ga('send', 'pageview');
</script>


  </body>

</html>
